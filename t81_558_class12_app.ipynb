{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Class 12: Deep Learning Applications**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tonight we will see how to apply deep learning networks to data science.  There are many applications of deep learning.  However, we will focus primarily upon data science.  For this class we will go beyond simple academic examples and see how to construct an ensemble that could potentially lead to a high score on a Kaggle competition.  We will see how to evaluate the importance of features and several ways to combine models.\n",
    "\n",
    "Tonights topics include:\n",
    "\n",
    "* Log Loss Error\n",
    "* Evaluating Feature Importance\n",
    "* The Biological Response Data Set\n",
    "* Neural Network Bagging\n",
    "* Nueral Network Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "These are exactly the same feature vector encoding functions from [Class 3](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class3_training.ipynb).  They must be defined for this class as well.  For more information, refer to class 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogLoss Error\n",
    "\n",
    "Log loss is an error metric that is often used in place of accuracy for classification.  Log loss allows for \"partial credit\" when a miss classification occurs.  For example, a model might be used to classify A, B and C.  The correct answer might be A, however if the classification network chose B as having the highest probability, then accuracy gives the neural network no credit for this classification.  \n",
    "\n",
    "However, with log loss, the probability of the correct answer is added to the score.  For example, the correct answer might be A, but if the neural network only predicted .8 probability of A being correct, then the value -log(.8) is added.\n",
    "\n",
    "$$ logloss = -\\frac{1}{N}\\sum^N_{i=1}\\sum^M_{j=1}y_{ij} \\log(\\hat{y}_{ij}) $$\n",
    "\n",
    "The following table shows the logloss scores that correspond to the average predicted accuracy for the correct item. The **pred** column specifies the average probability for the correct class.  The **logloss** column specifies the log loss for that probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.000000e-01</td>\n",
       "      <td>0.105361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>0.223144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.000000e-01</td>\n",
       "      <td>0.356675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.000000e-01</td>\n",
       "      <td>0.916291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>1.203973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.500000e-02</td>\n",
       "      <td>2.590267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>2.995732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.500000e-02</td>\n",
       "      <td>3.688879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>18.420681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pred    logloss\n",
       "0   1.000000e+00  -0.000000\n",
       "1   9.000000e-01   0.105361\n",
       "2   8.000000e-01   0.223144\n",
       "3   7.000000e-01   0.356675\n",
       "4   6.000000e-01   0.510826\n",
       "5   5.000000e-01   0.693147\n",
       "6   4.000000e-01   0.916291\n",
       "7   3.000000e-01   1.203973\n",
       "8   2.000000e-01   1.609438\n",
       "9   1.000000e-01   2.302585\n",
       "10  7.500000e-02   2.590267\n",
       "11  5.000000e-02   2.995732\n",
       "12  2.500000e-02   3.688879\n",
       "13  1.000000e-08  18.420681"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "loss = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.075, 0.05, 0.025, 1e-8 ]\n",
    "\n",
    "df = pd.DataFrame({'pred':loss, 'logloss': -np.log(loss)},columns=['pred','logloss'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the opposit.  For a given logloss, what is the average probability for the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logloss</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.904837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.818731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.740818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.670320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.606531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.548812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.496585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.449329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.406570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.223130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.135335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.082085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.049787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.030197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.018316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    logloss      pred\n",
       "0       0.1  0.904837\n",
       "1       0.2  0.818731\n",
       "2       0.3  0.740818\n",
       "3       0.4  0.670320\n",
       "4       0.5  0.606531\n",
       "5       0.6  0.548812\n",
       "6       0.7  0.496585\n",
       "7       0.8  0.449329\n",
       "8       0.9  0.406570\n",
       "9       1.0  0.367879\n",
       "10      1.5  0.223130\n",
       "11      2.0  0.135335\n",
       "12      2.5  0.082085\n",
       "13      3.0  0.049787\n",
       "14      3.5  0.030197\n",
       "15      4.0  0.018316"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "loss = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2, 2.5, 3, 3.5, 4 ]\n",
    "\n",
    "df = pd.DataFrame({'logloss':loss, 'pred': np.exp(np.negative(loss))},\n",
    "                  columns=['logloss','pred'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Sklearn Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScikitLearnTFDNNClassifier:\n",
    "    def __init__(self,name,hidden_units,num_classes,optimizer,steps):\n",
    "        self.classifier = None\n",
    "        self.model_dir = None\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_classes = num_classes\n",
    "        self.optimizer = optimizer\n",
    "        self.steps = steps\n",
    "        self.name = name\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"ScikitLearnTFDNNClassifier(name='{}',hidden_units={},num_classes={},optimizer={},steps={})\".format(\n",
    "        self.name,self.hidden_units,self.num_classes,self.optimizer,self.steps)\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        if self.classifier is None:\n",
    "            self.model_dir = get_model_dir(self.name,True)\n",
    "\n",
    "            # Create a deep neural network with 3 hidden layers of 10, 20, 5\n",
    "            feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[1])]\n",
    "            self.classifier = learn.DNNClassifier(\n",
    "                model_dir= self.model_dir,\n",
    "                config=tf.contrib.learn.RunConfig(save_checkpoints_secs=30),\n",
    "                hidden_units=self.hidden_units, n_classes=num_classes, feature_columns=feature_columns)\n",
    "\n",
    "        self.classifier.fit(x,y,steps=self.steps)\n",
    "        return self\n",
    "\n",
    "    def predict(self,x):\n",
    "        if self.classifier is None:\n",
    "            raise ValueError('A very specific bad thing happened')\n",
    "\n",
    "        return list(self.classifier.predict(x, as_iterable=True))\n",
    "\n",
    "    def predict_proba(self,x):\n",
    "        if self.classifier is None:\n",
    "            raise ValueError('A very specific bad thing happened')\n",
    "\n",
    "        return list(self.classifier.predict_proba(x))\n",
    "    \n",
    "class ScikitLearnTFDNNRegressor:\n",
    "    def __init__(self,name,hidden_units,optimizer,steps):\n",
    "        self.regressor = None\n",
    "        self.model_dir = None\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_classes = num_classes\n",
    "        self.optimizer = optimizer\n",
    "        self.steps = steps\n",
    "        self.name = name\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"ScikitLearnTFDNNRegressor(name='{}',hidden_units={},optimizer={},steps={})\".format(\n",
    "        self.name,self.hidden_units,self.optimizer,self.steps)\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        if self.regressor is None:\n",
    "            self.model_dir = get_model_dir(self.name,True)\n",
    "\n",
    "            # Create a deep neural network with 3 hidden layers of 10, 20, 5\n",
    "            feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[1])]\n",
    "            self.regressor = learn.DNNRegressor(\n",
    "                model_dir= self.model_dir,\n",
    "                config=tf.contrib.learn.RunConfig(save_checkpoints_secs=30),\n",
    "                hidden_units=self.hidden_units, feature_columns=feature_columns)\n",
    "\n",
    "        self.regressor.fit(x,y,steps=self.steps)\n",
    "        return self\n",
    "\n",
    "    def predict(self,x):\n",
    "        if self.regressor is None:\n",
    "            raise ValueError('A very specific bad thing happened')\n",
    "\n",
    "        return list(self.regressor.predict(x, as_iterable=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluating Feature Importance\n",
    "\n",
    "Feature importance tells us how important each of the features (from the feature/import vector are to the prediction of a neural network, or other model.  There are many different ways to evaluate feature importance for neural networks.  The following paper presents a very good (and readable) overview of the various means of evaluating the importance of neural network inputs/features.\n",
    "\n",
    "Olden, J. D., Joy, M. K., & Death, R. G. (2004). [An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data](http://depts.washington.edu/oldenlab/wordpress/wp-content/uploads/2013/03/EcologicalModelling_2004.pdf). *Ecological Modelling*, 178(3), 389-397.\n",
    "\n",
    "In summary, the following methods are available to neural networks:\n",
    "\n",
    "* Connection Weights Algorithm\n",
    "* Partial Derivatives\n",
    "* Input Perturbation\n",
    "* Sensitivity Analysis\n",
    "* Forward Stepwise Addition \n",
    "* Improved Stepwise Selection 1\n",
    "* Backward Stepwise Elimination\n",
    "* Improved Stepwise Selection\n",
    "\n",
    "For this class we will use the **Input Perturbation** feature ranking algorithm.  This algorithm will work with any regression or classification network.  implementation of the input perturbation algorithm for scikit-learn is given in the next section. This algorithm is implemented in a function below that will work with any scikit-learn model.\n",
    "\n",
    "This algorithm was introduced by [Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) in his seminal paper on random forests.  Although he presented this algorithm in conjunction with random forests, it is model-independent and appropriate for any supervised learning model.  This algorithm, known as the input perturbation algorithm, works by evaluating a trained model’s accuracy with each of the inputs individually shuffled from a data set.  Shuffling an input causes it to become useless—effectively removing it from the model. More important inputs will produce a less accurate score when they are removed by shuffling them. This process makes sense, because important features will contribute to the accuracy of the model.  The TensorFlow version of this algorithm is taken from the following paper.\n",
    "\n",
    "Heaton, J., McElwee, S., & Cannady, J. (May 2017). Early stabilizing feature importance for TensorFlow deep neural networks. In *International Joint Conference on Neural Networks (IJCNN 2017)* (accepted for publication). IEEE.\n",
    "\n",
    "This algorithm will use logloss to evaluate a classification problem and RMSE for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "def perturbation_rank(model, x, y, names, regression):\n",
    "    errors = []\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        hold = np.array(x[:, i])\n",
    "        np.random.shuffle(x[:, i])\n",
    "        \n",
    "        if regression:\n",
    "            pred = model.predict(x_test)\n",
    "            error = metrics.mean_squared_error(y, pred)\n",
    "        else:\n",
    "            pred = model.predict_proba(x)\n",
    "            error = metrics.log_loss(y, pred)\n",
    "            \n",
    "        errors.append(error)\n",
    "        x[:, i] = hold\n",
    "        \n",
    "    max_error = np.max(errors)\n",
    "    importance = [e/max_error for e in errors]\n",
    "\n",
    "    data = {'name':names,'error':errors,'importance':importance}\n",
    "    result = pd.DataFrame(data, columns = ['name','error','importance'])\n",
    "    result.sort_values(by=['importance'], ascending=[0], inplace=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00494: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x226b38aa7f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "path = \"./data/\"\n",
    "    \n",
    "filename = os.path.join(path,\"iris.csv\")    \n",
    "df = pd.read_csv(filename,na_values=['NA','?'])\n",
    "\n",
    "species = encode_text_index(df,\"species\")\n",
    "x,y = to_xy(df,\"species\")\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "model.fit(x,y,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/38 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>petal_l</td>\n",
       "      <td>1.251176</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petal_w</td>\n",
       "      <td>1.108034</td>\n",
       "      <td>0.885594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepal_w</td>\n",
       "      <td>0.176790</td>\n",
       "      <td>0.141299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sepal_l</td>\n",
       "      <td>0.160230</td>\n",
       "      <td>0.128063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name     error  importance\n",
       "2  petal_l  1.251176    1.000000\n",
       "3  petal_w  1.108034    0.885594\n",
       "1  sepal_w  0.176790    0.141299\n",
       "0  sepal_l  0.160230    0.128063"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = df.columns.values[0:-1] # x column names\n",
    "rank = perturbation_rank(model, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00136: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x226b3da5978>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank MPG fields\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "# create feature vector\n",
    "missing_median(df, 'horsepower')\n",
    "df.drop('name',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'horsepower')\n",
    "encode_numeric_zscore(df, 'weight')\n",
    "encode_numeric_zscore(df, 'cylinders')\n",
    "encode_numeric_zscore(df, 'displacement')\n",
    "encode_numeric_zscore(df, 'acceleration')\n",
    "encode_text_dummy(df, 'origin')\n",
    "\n",
    "# Encode to a 2D matrix for training\n",
    "x,y = to_xy(df,'mpg')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x,y,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acceleration</td>\n",
       "      <td>15.779439</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>horsepower</td>\n",
       "      <td>15.255577</td>\n",
       "      <td>0.966801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>displacement</td>\n",
       "      <td>13.450043</td>\n",
       "      <td>0.852378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cylinders</td>\n",
       "      <td>12.604163</td>\n",
       "      <td>0.798771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mpg</td>\n",
       "      <td>12.052859</td>\n",
       "      <td>0.763833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>origin-2</td>\n",
       "      <td>11.853414</td>\n",
       "      <td>0.751194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>year</td>\n",
       "      <td>10.955904</td>\n",
       "      <td>0.694315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weight</td>\n",
       "      <td>10.846774</td>\n",
       "      <td>0.687399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>origin-1</td>\n",
       "      <td>10.536667</td>\n",
       "      <td>0.667747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name      error  importance\n",
       "5  acceleration  15.779439    1.000000\n",
       "3    horsepower  15.255577    0.966801\n",
       "2  displacement  13.450043    0.852378\n",
       "1     cylinders  12.604163    0.798771\n",
       "0           mpg  12.052859    0.763833\n",
       "8      origin-2  11.853414    0.751194\n",
       "6          year  10.955904    0.694315\n",
       "4        weight  10.846774    0.687399\n",
       "7      origin-1  10.536667    0.667747"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = df.columns.values[0:-1] # x column names\n",
    "rank = perturbation_rank(model, x_test, y_test, names, True)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Biological Response Data Set\n",
    "\n",
    "* [Biological Response Dataset at Kaggle](https://www.kaggle.com/c/bioresponse)\n",
    "* [1st place interview for Boehringer Ingelheim Biological Response](http://blog.kaggle.com/2012/07/05/1st-place-interview-for-boehringer-ingelheim-biological-response/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_train = os.path.join(path,\"bio_train.csv\")\n",
    "filename_test = os.path.join(path,\"bio_test.csv\")\n",
    "filename_submit = os.path.join(path,\"bio_submit.csv\")\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "activity_classes = encode_text_index(df_train,'Activity')\n",
    "\n",
    "#display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3751, 1777)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Response with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting/Training...\n",
      "Epoch 00009: early stopping\n",
      "Fitting done...\n",
      "Validation logloss: 0.5504896390007364\n",
      "Validation accuracy score: 0.7761194029850746\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Encode feature vector\n",
    "x, y = to_xy(df_train,'Activity')\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "num_classes = len(activity_classes)\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42) \n",
    "\n",
    "print(\"Fitting/Training...\")\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "print(\"Fitting done...\")\n",
    "\n",
    "# Give logloss error\n",
    "pred = model.predict(x_test)\n",
    "pred2 = np.argmax(pred,axis=1)\n",
    "pred = pred[:,1]\n",
    "# Clip so that min is never exactly 0, max never 1\n",
    "pred = np.clip(pred,a_min=1e-6,a_max=(1-1e-6)) \n",
    "print(\"Validation logloss: {}\".format(sklearn.metrics.log_loss(y_test,pred)))\n",
    "\n",
    "# Evaluate success using accuracy\n",
    "pred_submit = pred.copy()\n",
    "y_test = y_test[:,1]\n",
    "score = metrics.accuracy_score(y_test, pred2)\n",
    "print(\"Validation accuracy score: {}\".format(score))\n",
    "\n",
    "# Build real submit file\n",
    "pred_submit = model.predict(x_submit)\n",
    "pred_submit = pred_submit[:,1]\n",
    "\n",
    "# Clip so that min is never exactly 0, max never 1\n",
    "pred = np.clip(pred,a_min=1e-6,a_max=(1-1e-6)) \n",
    "submit_df = pd.DataFrame({'MoleculeId':[x+1 for x in range(len(pred_submit))],'PredictedProbability':pred_submit})\n",
    "submit_df.to_csv(filename_submit, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Features/Columns are Important \n",
    "\n",
    "The following uses perturbation ranking to evaluate the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      "938/938 [==============================] - 0s     \n",
      " 32/938 [>.............................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>D26</td>\n",
       "      <td>0.595832</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>D50</td>\n",
       "      <td>0.565651</td>\n",
       "      <td>0.949347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>D1099</td>\n",
       "      <td>0.559131</td>\n",
       "      <td>0.938405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>D1048</td>\n",
       "      <td>0.556952</td>\n",
       "      <td>0.934747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>D1058</td>\n",
       "      <td>0.556426</td>\n",
       "      <td>0.933864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>D1035</td>\n",
       "      <td>0.555929</td>\n",
       "      <td>0.933030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>D1389</td>\n",
       "      <td>0.555795</td>\n",
       "      <td>0.932805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>D1025</td>\n",
       "      <td>0.555293</td>\n",
       "      <td>0.931963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>D1092</td>\n",
       "      <td>0.554741</td>\n",
       "      <td>0.931037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>D1187</td>\n",
       "      <td>0.554651</td>\n",
       "      <td>0.930886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>D1011</td>\n",
       "      <td>0.554531</td>\n",
       "      <td>0.930685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>D1070</td>\n",
       "      <td>0.554454</td>\n",
       "      <td>0.930556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>D960</td>\n",
       "      <td>0.554362</td>\n",
       "      <td>0.930400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>D1274</td>\n",
       "      <td>0.554320</td>\n",
       "      <td>0.930331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>D993</td>\n",
       "      <td>0.554304</td>\n",
       "      <td>0.930304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>D957</td>\n",
       "      <td>0.554229</td>\n",
       "      <td>0.930177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>D990</td>\n",
       "      <td>0.554226</td>\n",
       "      <td>0.930172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>D997</td>\n",
       "      <td>0.553972</td>\n",
       "      <td>0.929745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>D1108</td>\n",
       "      <td>0.553935</td>\n",
       "      <td>0.929684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>D1401</td>\n",
       "      <td>0.553817</td>\n",
       "      <td>0.929485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>D505</td>\n",
       "      <td>0.553574</td>\n",
       "      <td>0.929078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>D1008</td>\n",
       "      <td>0.553519</td>\n",
       "      <td>0.928985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>D1110</td>\n",
       "      <td>0.553454</td>\n",
       "      <td>0.928877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>D1036</td>\n",
       "      <td>0.553432</td>\n",
       "      <td>0.928840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>D1086</td>\n",
       "      <td>0.553368</td>\n",
       "      <td>0.928732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>D1001</td>\n",
       "      <td>0.553349</td>\n",
       "      <td>0.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>D1417</td>\n",
       "      <td>0.553297</td>\n",
       "      <td>0.928613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>D80</td>\n",
       "      <td>0.553249</td>\n",
       "      <td>0.928532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>D1391</td>\n",
       "      <td>0.553172</td>\n",
       "      <td>0.928403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>D1031</td>\n",
       "      <td>0.553161</td>\n",
       "      <td>0.928384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>D1169</td>\n",
       "      <td>0.548884</td>\n",
       "      <td>0.921207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>D1420</td>\n",
       "      <td>0.548861</td>\n",
       "      <td>0.921169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>D1453</td>\n",
       "      <td>0.548787</td>\n",
       "      <td>0.921045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>D1145</td>\n",
       "      <td>0.548740</td>\n",
       "      <td>0.920966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>D1303</td>\n",
       "      <td>0.548733</td>\n",
       "      <td>0.920953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D10</td>\n",
       "      <td>0.548717</td>\n",
       "      <td>0.920926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>D1419</td>\n",
       "      <td>0.548610</td>\n",
       "      <td>0.920746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>D1326</td>\n",
       "      <td>0.548604</td>\n",
       "      <td>0.920736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>D270</td>\n",
       "      <td>0.548599</td>\n",
       "      <td>0.920728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>D956</td>\n",
       "      <td>0.548571</td>\n",
       "      <td>0.920682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>D1285</td>\n",
       "      <td>0.548559</td>\n",
       "      <td>0.920660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>D126</td>\n",
       "      <td>0.548550</td>\n",
       "      <td>0.920647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>D1380</td>\n",
       "      <td>0.548507</td>\n",
       "      <td>0.920573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>D1762</td>\n",
       "      <td>0.548476</td>\n",
       "      <td>0.920522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>D952</td>\n",
       "      <td>0.548470</td>\n",
       "      <td>0.920511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>D1097</td>\n",
       "      <td>0.548465</td>\n",
       "      <td>0.920504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>D1746</td>\n",
       "      <td>0.548463</td>\n",
       "      <td>0.920501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>D980</td>\n",
       "      <td>0.548448</td>\n",
       "      <td>0.920475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>D1126</td>\n",
       "      <td>0.548354</td>\n",
       "      <td>0.920317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>D1416</td>\n",
       "      <td>0.548323</td>\n",
       "      <td>0.920265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>D985</td>\n",
       "      <td>0.548301</td>\n",
       "      <td>0.920228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>D1218</td>\n",
       "      <td>0.548054</td>\n",
       "      <td>0.919814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>D1680</td>\n",
       "      <td>0.547988</td>\n",
       "      <td>0.919703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>D1129</td>\n",
       "      <td>0.547864</td>\n",
       "      <td>0.919495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>D1173</td>\n",
       "      <td>0.547818</td>\n",
       "      <td>0.919418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>D1194</td>\n",
       "      <td>0.547780</td>\n",
       "      <td>0.919354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>D1341</td>\n",
       "      <td>0.547732</td>\n",
       "      <td>0.919273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>D1059</td>\n",
       "      <td>0.547515</td>\n",
       "      <td>0.918909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>D1065</td>\n",
       "      <td>0.547472</td>\n",
       "      <td>0.918836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>D1435</td>\n",
       "      <td>0.546916</td>\n",
       "      <td>0.917904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1776 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name     error  importance\n",
       "26      D26  0.595832    1.000000\n",
       "50      D50  0.565651    0.949347\n",
       "1099  D1099  0.559131    0.938405\n",
       "1048  D1048  0.556952    0.934747\n",
       "1058  D1058  0.556426    0.933864\n",
       "1035  D1035  0.555929    0.933030\n",
       "1389  D1389  0.555795    0.932805\n",
       "1025  D1025  0.555293    0.931963\n",
       "1092  D1092  0.554741    0.931037\n",
       "1187  D1187  0.554651    0.930886\n",
       "1011  D1011  0.554531    0.930685\n",
       "1070  D1070  0.554454    0.930556\n",
       "960    D960  0.554362    0.930400\n",
       "1274  D1274  0.554320    0.930331\n",
       "993    D993  0.554304    0.930304\n",
       "957    D957  0.554229    0.930177\n",
       "990    D990  0.554226    0.930172\n",
       "997    D997  0.553972    0.929745\n",
       "1108  D1108  0.553935    0.929684\n",
       "1401  D1401  0.553817    0.929485\n",
       "505    D505  0.553574    0.929078\n",
       "1008  D1008  0.553519    0.928985\n",
       "1110  D1110  0.553454    0.928877\n",
       "1036  D1036  0.553432    0.928840\n",
       "1086  D1086  0.553368    0.928732\n",
       "1001  D1001  0.553349    0.928700\n",
       "1417  D1417  0.553297    0.928613\n",
       "80      D80  0.553249    0.928532\n",
       "1391  D1391  0.553172    0.928403\n",
       "1031  D1031  0.553161    0.928384\n",
       "...     ...       ...         ...\n",
       "1169  D1169  0.548884    0.921207\n",
       "1420  D1420  0.548861    0.921169\n",
       "1453  D1453  0.548787    0.921045\n",
       "1145  D1145  0.548740    0.920966\n",
       "1303  D1303  0.548733    0.920953\n",
       "10      D10  0.548717    0.920926\n",
       "1419  D1419  0.548610    0.920746\n",
       "1326  D1326  0.548604    0.920736\n",
       "270    D270  0.548599    0.920728\n",
       "956    D956  0.548571    0.920682\n",
       "1285  D1285  0.548559    0.920660\n",
       "126    D126  0.548550    0.920647\n",
       "1380  D1380  0.548507    0.920573\n",
       "1762  D1762  0.548476    0.920522\n",
       "952    D952  0.548470    0.920511\n",
       "1097  D1097  0.548465    0.920504\n",
       "1746  D1746  0.548463    0.920501\n",
       "980    D980  0.548448    0.920475\n",
       "1126  D1126  0.548354    0.920317\n",
       "1416  D1416  0.548323    0.920265\n",
       "985    D985  0.548301    0.920228\n",
       "1218  D1218  0.548054    0.919814\n",
       "1680  D1680  0.547988    0.919703\n",
       "1129  D1129  0.547864    0.919495\n",
       "1173  D1173  0.547818    0.919418\n",
       "1194  D1194  0.547780    0.919354\n",
       "1341  D1341  0.547732    0.919273\n",
       "1059  D1059  0.547515    0.918909\n",
       "1065  D1065  0.547472    0.918836\n",
       "1435  D1435  0.546916    0.917904\n",
       "\n",
       "[1776 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = df_train.columns.values[0:-1] # x column names\n",
    "rank = perturbation_rank(model, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Response with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insample logloss: 0.1261067880812591\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn\n",
    "\n",
    "\n",
    "x, y = to_xy(df_train,'Activity')\n",
    "y = np.argmax(y,axis=1)\n",
    "x_test = df_test.as_matrix().astype(np.float32)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x, y)\n",
    "pred = rf.predict_proba(x_test)\n",
    "pred = pred[:,1]\n",
    "pred_insample = rf.predict_proba(x)\n",
    "pred_insample = pred_insample[:,1]\n",
    "\n",
    "submit_df = pd.DataFrame({'MoleculeId':[x+1 for x in range(len(pred))],'PredictedProbability':pred})\n",
    "submit_df.to_csv(filename_submit, index=False)\n",
    "print(\"Insample logloss: {}\".format(sklearn.metrics.log_loss(y,pred_insample)))\n",
    "#display(submit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>D27</td>\n",
       "      <td>0.185490</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>D88</td>\n",
       "      <td>0.152633</td>\n",
       "      <td>0.822862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>D146</td>\n",
       "      <td>0.145366</td>\n",
       "      <td>0.783690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>D747</td>\n",
       "      <td>0.144338</td>\n",
       "      <td>0.778146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>D659</td>\n",
       "      <td>0.142734</td>\n",
       "      <td>0.769497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>D66</td>\n",
       "      <td>0.142582</td>\n",
       "      <td>0.768678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>D91</td>\n",
       "      <td>0.141818</td>\n",
       "      <td>0.764559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>D78</td>\n",
       "      <td>0.141775</td>\n",
       "      <td>0.764327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>D18</td>\n",
       "      <td>0.139714</td>\n",
       "      <td>0.753219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D10</td>\n",
       "      <td>0.139666</td>\n",
       "      <td>0.752960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D14</td>\n",
       "      <td>0.139503</td>\n",
       "      <td>0.752077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D9</td>\n",
       "      <td>0.139412</td>\n",
       "      <td>0.751587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>D1036</td>\n",
       "      <td>0.139204</td>\n",
       "      <td>0.750469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>D106</td>\n",
       "      <td>0.139137</td>\n",
       "      <td>0.750104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>D95</td>\n",
       "      <td>0.138577</td>\n",
       "      <td>0.747087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>D107</td>\n",
       "      <td>0.138532</td>\n",
       "      <td>0.746846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>0.138396</td>\n",
       "      <td>0.746113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>D64</td>\n",
       "      <td>0.138143</td>\n",
       "      <td>0.744745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>D32</td>\n",
       "      <td>0.138129</td>\n",
       "      <td>0.744672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D17</td>\n",
       "      <td>0.138113</td>\n",
       "      <td>0.744586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D5</td>\n",
       "      <td>0.137902</td>\n",
       "      <td>0.743450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D16</td>\n",
       "      <td>0.137695</td>\n",
       "      <td>0.742334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>D89</td>\n",
       "      <td>0.137622</td>\n",
       "      <td>0.741938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>D469</td>\n",
       "      <td>0.137528</td>\n",
       "      <td>0.741433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>D200</td>\n",
       "      <td>0.137181</td>\n",
       "      <td>0.739560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>D21</td>\n",
       "      <td>0.136763</td>\n",
       "      <td>0.737306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D7</td>\n",
       "      <td>0.136552</td>\n",
       "      <td>0.736169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>D104</td>\n",
       "      <td>0.136448</td>\n",
       "      <td>0.735610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>D951</td>\n",
       "      <td>0.136341</td>\n",
       "      <td>0.735032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D6</td>\n",
       "      <td>0.136168</td>\n",
       "      <td>0.734101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>D571</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>D1694</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>D1695</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>D783</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>D787</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>D788</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>D726</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>D1776</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>D1663</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>D1474</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>D1452</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>D581</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>D1657</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>D1464</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>D1669</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>D1466</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>D1666</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>D499</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>D1664</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>D713</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>D1482</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>D1480</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>D1651</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>D1493</td>\n",
       "      <td>0.126107</td>\n",
       "      <td>0.679857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>D1616</td>\n",
       "      <td>0.126106</td>\n",
       "      <td>0.679855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>D300</td>\n",
       "      <td>0.126104</td>\n",
       "      <td>0.679843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>D1774</td>\n",
       "      <td>0.126104</td>\n",
       "      <td>0.679843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>D1693</td>\n",
       "      <td>0.126104</td>\n",
       "      <td>0.679842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>D1550</td>\n",
       "      <td>0.126104</td>\n",
       "      <td>0.679841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>D781</td>\n",
       "      <td>0.126104</td>\n",
       "      <td>0.679841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1776 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name     error  importance\n",
       "26      D27  0.185490    1.000000\n",
       "87      D88  0.152633    0.822862\n",
       "145    D146  0.145366    0.783690\n",
       "746    D747  0.144338    0.778146\n",
       "658    D659  0.142734    0.769497\n",
       "65      D66  0.142582    0.768678\n",
       "90      D91  0.141818    0.764559\n",
       "77      D78  0.141775    0.764327\n",
       "17      D18  0.139714    0.753219\n",
       "9       D10  0.139666    0.752960\n",
       "13      D14  0.139503    0.752077\n",
       "8        D9  0.139412    0.751587\n",
       "1035  D1036  0.139204    0.750469\n",
       "105    D106  0.139137    0.750104\n",
       "94      D95  0.138577    0.747087\n",
       "106    D107  0.138532    0.746846\n",
       "1        D2  0.138396    0.746113\n",
       "63      D64  0.138143    0.744745\n",
       "31      D32  0.138129    0.744672\n",
       "16      D17  0.138113    0.744586\n",
       "4        D5  0.137902    0.743450\n",
       "15      D16  0.137695    0.742334\n",
       "88      D89  0.137622    0.741938\n",
       "468    D469  0.137528    0.741433\n",
       "199    D200  0.137181    0.739560\n",
       "20      D21  0.136763    0.737306\n",
       "6        D7  0.136552    0.736169\n",
       "103    D104  0.136448    0.735610\n",
       "950    D951  0.136341    0.735032\n",
       "5        D6  0.136168    0.734101\n",
       "...     ...       ...         ...\n",
       "570    D571  0.126107    0.679858\n",
       "1693  D1694  0.126107    0.679858\n",
       "1694  D1695  0.126107    0.679858\n",
       "782    D783  0.126107    0.679858\n",
       "786    D787  0.126107    0.679858\n",
       "787    D788  0.126107    0.679858\n",
       "725    D726  0.126107    0.679858\n",
       "1775  D1776  0.126107    0.679858\n",
       "1662  D1663  0.126107    0.679858\n",
       "1473  D1474  0.126107    0.679858\n",
       "1451  D1452  0.126107    0.679858\n",
       "580    D581  0.126107    0.679858\n",
       "1656  D1657  0.126107    0.679858\n",
       "1463  D1464  0.126107    0.679858\n",
       "1668  D1669  0.126107    0.679858\n",
       "1465  D1466  0.126107    0.679858\n",
       "1665  D1666  0.126107    0.679858\n",
       "498    D499  0.126107    0.679858\n",
       "1663  D1664  0.126107    0.679858\n",
       "712    D713  0.126107    0.679858\n",
       "1481  D1482  0.126107    0.679858\n",
       "1479  D1480  0.126107    0.679858\n",
       "1650  D1651  0.126107    0.679857\n",
       "1492  D1493  0.126107    0.679857\n",
       "1615  D1616  0.126106    0.679855\n",
       "299    D300  0.126104    0.679843\n",
       "1773  D1774  0.126104    0.679843\n",
       "1692  D1693  0.126104    0.679842\n",
       "1549  D1550  0.126104    0.679841\n",
       "780    D781  0.126104    0.679841\n",
       "\n",
       "[1776 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = df_train.columns.values[1:] # x column names\n",
    "rank = perturbation_rank(rf, x, y, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Bagging\n",
    "\n",
    "Neural networks will typically achieve better results when they are bagged.  Bagging a neural network is a process where the same neural network is trained over and over and the results are averaged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeffh\\Anaconda3\\envs\\wustl\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Model: 0 : <keras.wrappers.scikit_learn.KerasClassifier object at 0x00000226B5262A90>\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6482     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5387     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4807     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4485     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4296     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4107     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4058     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3898     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3857     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3700     \n",
      "2368/2501 [===========================>..] - ETA: 0sFold #0: loss=0.5387259995504785\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6589     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5599     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4971     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4649     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4406     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4294     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4130     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4016     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3956     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3842     \n",
      "2496/2501 [============================>.] - ETA: 0sFold #1: loss=0.4777925638168707\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6700     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5834     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5103     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4687     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4439     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4239     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4092     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3941     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3858     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3763     \n",
      "2208/2501 [=========================>....] - ETA: 0s ETAFold #2: loss=0.5765384475651304\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6841     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6123     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5230     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4823     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4537     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4390     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4242     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4136     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4053     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4015     \n",
      "2432/2501 [============================>.] - ETA: 0sFold #3: loss=0.47959218289317557\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6642     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5702     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5019     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4657     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4396     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4247     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4133     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4026     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3906     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3837     \n",
      "2432/2501 [============================>.] - ETA: 0sFold #4: loss=0.5475821176438681\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6727     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5751     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4998     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4609     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4372     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4214     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4082     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4010     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3875     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3824     \n",
      "2368/2501 [===========================>..] - ETA: 0sFold #5: loss=0.6601610968179055\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6413     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5342     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4780     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4472     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4336     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4129     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3980     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3935     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3767     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3668     \n",
      "2272/2501 [==========================>...] - ETA: 0sFold #6: loss=0.47840636377281787\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6682     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5739     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5011     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4677     - ETA: 0s - loss: \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4440     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4244     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4152     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4054     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3947     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3858     \n",
      "2304/2501 [==========================>...] - ETA: 0sFold #7: loss=0.5025782943284559\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 1s - loss: 0.6591     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5481     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4880     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4565     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4352     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4156     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3999     - ETA: 0s - loss: \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3886     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3802     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3684     \n",
      "2400/2501 [===========================>..] - ETA: 0sFold #8: loss=0.5124012921624626\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6520     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5455     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4843     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4482     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4229     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4099     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3918     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3794     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3623     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3537     \n",
      "2496/2501 [============================>.] - ETA: 0sFold #9: loss=0.566150889706028\n",
      "KerasClassifier: Mean loss=0.5339929248257194\n",
      "Model: 1 : <keras.wrappers.scikit_learn.KerasClassifier object at 0x00000226B544C0B8>\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 1s - loss: 0.6323     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5246     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4639     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4367     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4184     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4000     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3835     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3721     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3622     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3487     \n",
      "2112/2501 [========================>.....] - ETA: 0sFold #0: loss=0.5722266215475992\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6411     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5301     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4723     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4433     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4224     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4039     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3929     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3786     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3656     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3498     \n",
      "2080/2501 [=======================>......] - ETA: 0sFold #1: loss=0.47309980630287\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 1s - loss: 0.6351     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5201     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4656     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4307     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4118     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3952     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3833     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3609     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3536     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3413     \n",
      "2501/2501 [==============================] - 0s     \n",
      "Fold #2: loss=0.5936626383352256\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 1s - loss: 0.6470     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5350     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4805     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4491     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4232     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4050     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3918     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3779     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3650     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3533     \n",
      "2336/2501 [===========================>..] - ETA: 0sFold #3: loss=0.48220360717320787\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 1s - loss: 0.6738     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5635     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4928     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4577     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4355     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4201     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4017     - ETA: 0s - loss:\n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3928     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3800     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3745     \n",
      "2208/2501 [=========================>....] - ETA: 0sFold #4: loss=0.5606626219412981\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6296     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5060     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4557     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4268     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4062     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3939     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3785     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3637     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3531     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3531     \n",
      "2080/2501 [=======================>......] - ETA: 0sFold #5: loss=0.6610511467629491\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6426     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5399     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4796     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4452     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4268     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4100     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3951     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3910     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3728     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3588     \n",
      "2464/2501 [============================>.] - ETA: 0sFold #6: loss=0.4641760069637206\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6666     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5553     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4866     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4560     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4365     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4204     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4127     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3991     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3914     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3829     \n",
      "2272/2501 [==========================>...] - ETA: 0sFold #7: loss=0.50203265397229\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6410     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5245     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4745     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4409     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4195     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4001     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3876     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3754     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3614     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3537     \n",
      "2272/2501 [==========================>...] - ETA: 0sFold #8: loss=0.49406496336106037\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 1s - loss: 0.6308     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5154     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4684     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4357     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4156     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4021     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3870     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3693     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3569     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3395     \n",
      "2432/2501 [============================>.] - ETA: 0sFold #9: loss=0.5816344104611996\n",
      "KerasClassifier: Mean loss=0.538481447682142\n",
      "Model: 2 : <keras.wrappers.scikit_learn.KerasClassifier object at 0x00000226B544C0F0>\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6297     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5121     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4615     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4275     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4087     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3862     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3662     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3495     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3284     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3197     \n",
      "2336/2501 [===========================>..] - ETA: 0sFold #0: loss=0.5223477423493172\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6456     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5296     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4744     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4457     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4220     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4021     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3868     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3716     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3589     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3462     \n",
      "2464/2501 [============================>.] - ETA: 0sFold #1: loss=0.47317563846334443\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6428     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5253     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4595     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4274     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4024     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3879     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3683     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3487     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3347     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3222     \n",
      "2048/2501 [=======================>......] - ETA: 0sFold #2: loss=0.6540460654256409\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6629     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5425     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4821     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4482     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4376     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4184     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4068     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3949     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3902     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3830     \n",
      "376/376 [==============================] - 0s     \n",
      "2400/2501 [===========================>..] - ETA: 0sFold #3: loss=0.487692541033737\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 2s - loss: 0.6132     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5076     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4469     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4290     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3956     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3700     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3528     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3323     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3092     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.2903     \n",
      "2501/2501 [==============================] - 0s     \n",
      "Fold #4: loss=0.589925765311471\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6731     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5550     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4821     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4483     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4268     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4069     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3979     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3914     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3760     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3711     \n",
      "2432/2501 [============================>.] - ETA: 0sFold #5: loss=0.6647946662838016\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 1s - loss: 0.6035     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4952     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4539     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4238     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4001     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3859     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3672     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3481     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3388     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3125     \n",
      "2208/2501 [=========================>....] - ETA: 0sFold #6: loss=0.490776271172569\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 1s - loss: 0.6441     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5331     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4747     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4442     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4230     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4085     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3888     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3794     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3705     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3561     \n",
      "2432/2501 [============================>.] - ETA: 0sFold #7: loss=0.5410842681424806\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 1s - loss: 0.6257     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5125     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4632     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 1s - loss: 0.4314     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4116     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3966     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3854     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3629     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3478     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3322     \n",
      "2432/2501 [============================>.] - ETA: 0sFold #8: loss=0.5116624967509116\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 1s - loss: 0.6396     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5263     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4684     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4373     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4183     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3991     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3879     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3726     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3651     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3504     \n",
      "2336/2501 [===========================>..] - ETA: 0sFold #9: loss=0.5895825173946888\n",
      "KerasClassifier: Mean loss=0.5525087972327963\n",
      "\n",
      "Blending models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "PATH = \"./data/\"\n",
    "SHUFFLE = False\n",
    "FOLDS = 10\n",
    "\n",
    "def build_ann(input_size,classes,neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(classes,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def stretch(y):\n",
    "    return (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "def blend_ensemble(x, y, x_submit):\n",
    "\n",
    "    folds = list(StratifiedKFold(y, FOLDS))\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "\n",
    "    models = [\n",
    "        KerasClassifier(build_fn=build_ann,neurons=10,input_size=x.shape[1],classes=2),\n",
    "        KerasClassifier(build_fn=build_ann,neurons=20,input_size=x.shape[1],classes=2),\n",
    "        KerasClassifier(build_fn=build_ann,neurons=30,input_size=x.shape[1],classes=2)\n",
    "        ]\n",
    "\n",
    "    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model: {} : {}\".format(j, model) )\n",
    "        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
    "        total_loss = 0\n",
    "        for i, (train, test) in enumerate(folds):\n",
    "            x_train = x[train]\n",
    "            y_train = y[train]\n",
    "            x_test = x[test]\n",
    "            y_test = y[test]\n",
    "            model.fit(x_train, y_train)\n",
    "            pred = np.array(model.predict_proba(x_test))\n",
    "            # pred = model.predict_proba(x_test)\n",
    "            dataset_blend_train[test, j] = pred[:, 1]\n",
    "            pred2 = np.array(model.predict_proba(x_submit))\n",
    "            #fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n",
    "            fold_sums[:, i] = pred2[:, 1]\n",
    "            loss = mlogloss(y_test, pred)\n",
    "            total_loss+=loss\n",
    "            print(\"Fold #{}: loss={}\".format(i,loss))\n",
    "        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
    "        dataset_blend_test[:, j] = fold_sums.mean(1)\n",
    "\n",
    "    print()\n",
    "    print(\"Blending models.\")\n",
    "    blend = LogisticRegression()\n",
    "    blend.fit(dataset_blend_train, y)\n",
    "    return blend.predict_proba(dataset_blend_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(42)  # seed to shuffle the train set\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    filename_train = os.path.join(PATH, \"bio_train.csv\")\n",
    "    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n",
    "\n",
    "    filename_submit = os.path.join(PATH, \"bio_test.csv\")\n",
    "    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n",
    "\n",
    "    predictors = list(df_train.columns.values)\n",
    "    predictors.remove('Activity')\n",
    "    x = df_train.as_matrix(predictors)\n",
    "    y = df_train['Activity']\n",
    "    x_submit = df_submit.as_matrix()\n",
    "\n",
    "    if SHUFFLE:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    submit_data = blend_ensemble(x, y, x_submit)\n",
    "    submit_data = stretch(submit_data)\n",
    "\n",
    "    ####################\n",
    "    # Build submit file\n",
    "    ####################\n",
    "    ids = [id+1 for id in range(submit_data.shape[0])]\n",
    "    submit_filename = os.path.join(PATH, \"bio_submit.csv\")\n",
    "    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n",
    "                             columns=['MoleculeId','PredictedProbability'])\n",
    "    submit_df.to_csv(submit_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Ensemble\n",
    "\n",
    "A neural network ensemble combines neural network predictions with other models.  The exact blend of all of these models is determined by logistic regression.  The following code performs this blend for a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Model: 0 : <keras.wrappers.scikit_learn.KerasClassifier object at 0x00000226B3FF6898>\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 1s - loss: 0.6450     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5298     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4709     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4391     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4192     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3982     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3914     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3731     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3688     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3438     \n",
      "2400/2501 [===========================>..] - ETA: 0sFold #0: loss=0.5338064958666865\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 1s - loss: 0.6497     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5415     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4818     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4493     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4233     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4145     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3937     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3794     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3705     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3597     \n",
      "2208/2501 [=========================>....] - ETA: 0sFold #1: loss=0.4724153796394353\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 1s - loss: 0.6564     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5539     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4865     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4495     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4295     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4120     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3988     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3846     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3748     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3672     \n",
      "2432/2501 [============================>.] - ETA: 0sFold #2: loss=0.5846489944538747\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 1s - loss: 0.6784     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5840     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5049     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4698     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4426     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4293     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4151     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4049     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3970     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3931     \n",
      "2432/2501 [============================>.] - ETA: 0sFold #3: loss=0.4893697113565435\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 1s - loss: 0.6546     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5560     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4867     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4529     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4259     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4099     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3979     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3853     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3705     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3633     \n",
      "2368/2501 [===========================>..] - ETA: 0sFold #4: loss=0.5600915677818187\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 1s - loss: 0.6686     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5623     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4884     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4529     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4297     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4154     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4016     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3936     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3801     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3729     \n",
      "2464/2501 [============================>.] - ETA: 0sFold #5: loss=0.6752410948840426\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 1s - loss: 0.6250     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5098     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4596     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4275     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4120     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3868     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3680     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3602     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3364     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3236     \n",
      "2400/2501 [===========================>..] - ETA: 0sFold #6: loss=0.5035587784481742\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 2s - loss: 0.6601     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 1s - loss: 0.5550     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4898     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4610     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4356     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4162     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4077     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3963     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3857     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3749     \n",
      "2304/2501 [==========================>...] - ETA: 0sFold #7: loss=0.4923696836051568\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 1s - loss: 0.6248     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5227     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4683     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4469     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4199     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4019     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3811     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3679     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3597     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3452     \n",
      "2336/2501 [===========================>..] - ETA: 0sFold #8: loss=0.5094752574124143\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 1s - loss: 0.6433     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5259     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4660     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4337     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4062     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3880     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3673     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3568     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3334     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3249     \n",
      "2400/2501 [===========================>..] - ETA: 0sFold #9: loss=0.5894619616228683\n",
      "KerasClassifier: Mean loss=0.5410438925071015\n",
      "Model: 1 : KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Fold #0: loss=3.606678388314123\n",
      "Fold #1: loss=2.2197228940978317\n",
      "Fold #2: loss=3.6717523663107237\n",
      "Fold #3: loss=2.5045156203944594\n",
      "Fold #4: loss=4.443553550438037\n",
      "Fold #5: loss=4.410524301688227\n",
      "Fold #6: loss=3.400455469543658\n",
      "Fold #7: loss=3.0885474338547683\n",
      "Fold #8: loss=2.1219335323249253\n",
      "Fold #9: loss=3.0613772690497245\n",
      "KNeighborsClassifier: Mean loss=3.2529060826016476\n",
      "Model: 2 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.45859387026544035\n",
      "Fold #1: loss=0.43226994423427495\n",
      "Fold #2: loss=0.4762818884046444\n",
      "Fold #3: loss=0.43338810350051965\n",
      "Fold #4: loss=0.47502753371869\n",
      "Fold #5: loss=0.492658174605396\n",
      "Fold #6: loss=0.4015988631740329\n",
      "Fold #7: loss=0.4717953689427381\n",
      "Fold #8: loss=0.4517105069522408\n",
      "Fold #9: loss=0.46971357575792544\n",
      "RandomForestClassifier: Mean loss=0.45630378295559015\n",
      "Model: 3 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.4501929416405849\n",
      "Fold #1: loss=0.43527263084022183\n",
      "Fold #2: loss=0.4698774736941605\n",
      "Fold #3: loss=0.41784016293793536\n",
      "Fold #4: loss=0.470862719005535\n",
      "Fold #5: loss=0.5007009099421501\n",
      "Fold #6: loss=0.40496551263998337\n",
      "Fold #7: loss=0.4688833748405199\n",
      "Fold #8: loss=0.45798694087339165\n",
      "Fold #9: loss=0.45974118174298945\n",
      "RandomForestClassifier: Mean loss=0.45363238481574725\n",
      "Model: 4 : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.45131247798132806\n",
      "Fold #1: loss=0.4230980001099943\n",
      "Fold #2: loss=0.586187893928544\n",
      "Fold #3: loss=0.42553475374872446\n",
      "Fold #4: loss=0.4964472681005012\n",
      "Fold #5: loss=0.494729595836481\n",
      "Fold #6: loss=0.41487748895885895\n",
      "Fold #7: loss=0.48506699974271766\n",
      "Fold #8: loss=0.4595462862215204\n",
      "Fold #9: loss=0.4673771268612419\n",
      "ExtraTreesClassifier: Mean loss=0.47041778914899124\n",
      "Model: 5 : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.44916718203909234\n",
      "Fold #1: loss=0.4141554387911032\n",
      "Fold #2: loss=0.5897250522492203\n",
      "Fold #3: loss=0.4234716042493701\n",
      "Fold #4: loss=0.49267251836808157\n",
      "Fold #5: loss=0.5029294613500651\n",
      "Fold #6: loss=0.4138860223390099\n",
      "Fold #7: loss=0.6423108659854284\n",
      "Fold #8: loss=0.5382143341356395\n",
      "Fold #9: loss=0.6301460999525225\n",
      "ExtraTreesClassifier: Mean loss=0.5096678579459533\n",
      "Model: 6 : GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=6,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=50, presort='auto', random_state=None,\n",
      "              subsample=0.5, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.4825902868109151\n",
      "Fold #1: loss=0.46358777326561157\n",
      "Fold #2: loss=0.47459513232119027\n",
      "Fold #3: loss=0.4429692033478653\n",
      "Fold #4: loss=0.5016500178201619\n",
      "Fold #5: loss=0.49482269441064036\n",
      "Fold #6: loss=0.4523073165135777\n",
      "Fold #7: loss=0.4599996351177036\n",
      "Fold #8: loss=0.45310725017503284\n",
      "Fold #9: loss=0.47101543797655887\n",
      "GradientBoostingClassifier: Mean loss=0.4696644747759257\n",
      "\n",
      "Blending models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "PATH = \"./data/\"\n",
    "SHUFFLE = False\n",
    "FOLDS = 10\n",
    "\n",
    "def build_ann(input_size,classes,neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(classes,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def stretch(y):\n",
    "    return (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "def blend_ensemble(x, y, x_submit):\n",
    "\n",
    "    folds = list(StratifiedKFold(y, FOLDS))\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "\n",
    "    models = [\n",
    "        KerasClassifier(build_fn=build_ann,neurons=20,input_size=x.shape[1],classes=2),\n",
    "        KNeighborsClassifier(n_neighbors=3),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model: {} : {}\".format(j, model) )\n",
    "        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
    "        total_loss = 0\n",
    "        for i, (train, test) in enumerate(folds):\n",
    "            x_train = x[train]\n",
    "            y_train = y[train]\n",
    "            x_test = x[test]\n",
    "            y_test = y[test]\n",
    "            model.fit(x_train, y_train)\n",
    "            pred = np.array(model.predict_proba(x_test))\n",
    "            # pred = model.predict_proba(x_test)\n",
    "            dataset_blend_train[test, j] = pred[:, 1]\n",
    "            pred2 = np.array(model.predict_proba(x_submit))\n",
    "            #fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n",
    "            fold_sums[:, i] = pred2[:, 1]\n",
    "            loss = mlogloss(y_test, pred)\n",
    "            total_loss+=loss\n",
    "            print(\"Fold #{}: loss={}\".format(i,loss))\n",
    "        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
    "        dataset_blend_test[:, j] = fold_sums.mean(1)\n",
    "\n",
    "    print()\n",
    "    print(\"Blending models.\")\n",
    "    blend = LogisticRegression()\n",
    "    blend.fit(dataset_blend_train, y)\n",
    "    return blend.predict_proba(dataset_blend_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(42)  # seed to shuffle the train set\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    filename_train = os.path.join(PATH, \"bio_train.csv\")\n",
    "    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n",
    "\n",
    "    filename_submit = os.path.join(PATH, \"bio_test.csv\")\n",
    "    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n",
    "\n",
    "    predictors = list(df_train.columns.values)\n",
    "    predictors.remove('Activity')\n",
    "    x = df_train.as_matrix(predictors)\n",
    "    y = df_train['Activity']\n",
    "    x_submit = df_submit.as_matrix()\n",
    "\n",
    "    if SHUFFLE:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    submit_data = blend_ensemble(x, y, x_submit)\n",
    "    submit_data = stretch(submit_data)\n",
    "\n",
    "    ####################\n",
    "    # Build submit file\n",
    "    ####################\n",
    "    ids = [id+1 for id in range(submit_data.shape[0])]\n",
    "    submit_filename = os.path.join(PATH, \"bio_submit.csv\")\n",
    "    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n",
    "                             columns=['MoleculeId','PredictedProbability'])\n",
    "    submit_df.to_csv(submit_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
